{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data\n",
    "human:https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n",
    "dog: https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 total human images.\n",
      "There are 0 total dog images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames for human and dog images\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are {} total human images.'.format(len(human_files)))\n",
    "print('There are {} total dog images.'.format(len(dog_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5590eae49e69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# load color (BGR) image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# convert BGR image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[0])\n",
    "\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img, (x,y), (x + w, y + h), (255,0,0), 2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3199c664070047d0b9b3d0bf06466d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Humans', max=1, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec03347bdad4676b4962e0e54ba2c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dogs', max=1, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-12e63cdde368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mincorrect_face_tally\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdog_files_short\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Dogs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{sum(face_tally) / len(human_files_short) * 100:4.1f}% of humans correctly classified'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{sum(incorrect_face_tally) / len(dog_files_short) * 100:4.1f}% of dogs missclassified'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "human_files_short = human_files[:100]\n",
    "dog_files_short = dog_files[:100]\n",
    "\n",
    "#-#-# Do NOT modify the code above this line. #-#-#\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "face_tally = [face_detector(face) for face in tqdm_notebook(human_files_short, desc='Humans')]\n",
    "incorrect_face_tally = [face_detector(face) for face in tqdm_notebook(dog_files_short, desc='Dogs')]\n",
    "\n",
    "print(f'{sum(face_tally) / len(human_files_short) * 100:4.1f}% of humans correctly classified')\n",
    "print(f'{sum(incorrect_face_tally) / len(dog_files_short) * 100:4.1f}% of dogs missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "from src import detect_faces, show_bboxes\n",
    "\n",
    "image = Image.open(human_files_short[0])\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(image, min_face_size=90.0)\n",
    "plt.imshow(show_bboxes(image, bounding_boxes, landmarks))\n",
    "plt.show()\n",
    "print('Number of faces detected:', len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_face_detector(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    bounding_boxes, landmarks = detect_faces(img, min_face_size=90.0)    \n",
    "    return len(bounding_boxes) > 0\n",
    "\n",
    "face_tally = [nn_face_detector(file) for file in tqdm_notebook(human_files_short, desc='Humans')]\n",
    "incorrect_face_tally = [nn_face_detector(file) for file in tqdm_notebook(dog_files_short, desc='Dogs')]\n",
    "\n",
    "print(f'{sum(face_tally) / len(human_files_short) * 100:4.1f}% of humans correctly classified')\n",
    "print(f'{sum(incorrect_face_tally) / len(dog_files_short) * 100:4.1f}% of dogs missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# define VGG16 model\n",
    "VGG16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "\n",
    "    '''\n",
    "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
    "    predicted ImageNet class for image at specified path\n",
    "    \n",
    "    Args:\n",
    "        img_path: path to an image\n",
    "        \n",
    "    Returns:\n",
    "        Index corresponding to VGG-16 model's prediction\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Complete the function.\n",
    "    ## Load and pre-process an image from the given img_path\n",
    "    ## Return the *index* of the predicted class for that image\n",
    "    \n",
    "\n",
    "    # Import image from img_path in PIL format\n",
    "    img = Image.open(img_path)\n",
    "   \n",
    "    # Define normalization step for image\n",
    "    normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225))\n",
    "\n",
    "    # Define transformations of image\n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize])\n",
    "\n",
    "    # Preprocess image to 4D Tensor (.unsqueeze(0) adds a dimension)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "\n",
    "    # Move tensor to GPU if available\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.cuda()\n",
    "        \n",
    "    ## Inference\n",
    "    # Turn on evaluation mode\n",
    "    VGG16.eval()\n",
    "    \n",
    "    # Get predicted category for image\n",
    "    with torch.no_grad():\n",
    "        output = VGG16(img_tensor)\n",
    "        prediction = torch.argmax(output).item()\n",
    "        \n",
    "    # Turn off evaluation mode\n",
    "    VGG16.train()\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_detector(img_path):\n",
    "    ## TODO: Complete the function.\n",
    "    prediction = VGG16_predict(img_path)\n",
    "    return True if 151 <= prediction <= 268 else False # true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_in_human_files_VGG16 = 0\n",
    "\n",
    "for file in tqdm_notebook(human_files_short, desc='human_files'):\n",
    "    if dog_detector(file):\n",
    "        dogs_in_human_files_VGG16 += 1\n",
    "\n",
    "# dog_files_short\n",
    "dogs_in_dog_files_VGG16 = 0\n",
    "\n",
    "for file in tqdm_notebook(dog_files_short, desc='dog_files'):\n",
    "    if dog_detector(file):\n",
    "        dogs_in_dog_files_VGG16 += 1\n",
    "\n",
    "print('#### VGG16 ####')\n",
    "print(f'Dogs detected in \"human_files_short\": {dogs_in_human_files_VGG16 / len(human_files_short) * 100}%')\n",
    "print(f'Dogs detected in \"dog_files_short\": {dogs_in_dog_files_VGG16 / len(dog_files_short) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "if use_cuda:\n",
    "    ResNet50.cuda()\n",
    "\n",
    "# Performance variables\n",
    "dogs_in_human_files_ResNet50 = 0\n",
    "dogs_in_dog_files_ResNet50 = 0\n",
    "\n",
    "# Preprocess definitions\n",
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])\n",
    "# Turn on evaluation mode\n",
    "ResNet50.eval()\n",
    "\n",
    "# Test performance of ResNet50 model on human_files_short\n",
    "for img_path in tqdm_notebook(human_files_short, desc='human_files'):\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ResNet50 = ResNet50(img_tensor)\n",
    "        prediction_ResNet50 = torch.argmax(output_ResNet50).item()\n",
    "        if 151 <= prediction_ResNet50 <= 268:\n",
    "            dogs_in_human_files_ResNet50 += 1\n",
    "            \n",
    "# Test performance of ResNet50 model on dogs_files_short\n",
    "for img_path in tqdm_notebook(dog_files_short, desc='dog_files'):\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "    if use_cuda:\n",
    "        img_tensor = image_tensor.cuda\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ResNet50 = ResNet50(img_tensor)\n",
    "        prediction_ResNet50 = torch.argmax(output_ResNet50).item()\n",
    "        if 151 <= prediction_ResNet50 <= 268:\n",
    "            dogs_in_dog_files_ResNet50 += 1\n",
    "\n",
    "ResNet50.train()\n",
    "\n",
    "print('#### ResNet50 ####')\n",
    "print(f'Dogs detected in \"human_files_short\": {dogs_in_human_files_ResNet50 / len(human_files_short) * 100}%')\n",
    "print(f'Dogs detected in \"dog_files_short\": {dogs_in_dog_files_ResNet50 / len(human_files_short) * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InceptionV3 = models.inception_v3(pretrained=True)\n",
    "\n",
    "if use_cuda:\n",
    "    InceptionV3.cuda()\n",
    "\n",
    "dogs_in_human_files_InceptionV3 = 0\n",
    "dogs_in_dog_files_InceptionV3 = 0\n",
    "\n",
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "# CAVE: InceptionV3 needs input tensor to be (3, 299, 299)\n",
    "preprocess = transforms.Compose([transforms.Resize(320),\n",
    "                                 transforms.CenterCrop(299),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])\n",
    "\n",
    "InceptionV3.eval()\n",
    "\n",
    "for img_path in tqdm_notebook(human_files_short, desc='human_files'):\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_InceptionV3 = InceptionV3(img_tensor)\n",
    "        prediction_InceptionV3 = torch.argmax(output_InceptionV3).item()\n",
    "        if 151 <= prediction_InceptionV3 <= 268:\n",
    "            dogs_in_human_files_InceptionV3 += 1\n",
    "            \n",
    "for img_path in tqdm_notebook(dog_files_short, desc='dog_files'):\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "    if use_cuda:\n",
    "        img_tensor = image_tensor.cuda\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_InceptionV3 = InceptionV3(img_tensor)\n",
    "        prediction_InceptionV3 = torch.argmax(output_InceptionV3).item()\n",
    "        if 151 <= prediction_InceptionV3 <= 268:\n",
    "            dogs_in_dog_files_InceptionV3 += 1\n",
    "\n",
    "InceptionV3.train()\n",
    "\n",
    "print('#### InceptionV3 ####')\n",
    "print(f'Dogs detected in \"human_files_short\": {dogs_in_human_files_InceptionV3 / len(human_files_short) * 100}%')\n",
    "print(f'ResNet50Dogs detected in \"dog_files_short\": {dogs_in_dog_files_InceptionV3 / len(human_files_short) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "data_dir = 'dogImages'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize(size=258),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomRotation(10),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "\n",
    "validTest_transforms = transforms.Compose([transforms.Resize(size=258),\n",
    "                                           transforms.CenterCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                                [0.5, 0.5, 0.5])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=validTest_transforms)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=validTest_transforms)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "validLoader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(25088, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 133)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = x.view(-1, 25088)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#-#-# You so NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            \n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "\n",
    "            # clear gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forwarward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # calculate batch loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # perform optimization step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update training loss\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))            \n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "           \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# define loaders_scratch\n",
    "loaders_scratch = {'train': trainLoader,\n",
    "                   'valid': validLoader,\n",
    "                   'test': testLoader}\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(25, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)       \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        output = F.softmax(output, dim=1)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dogImages'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize(size=258),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomRotation(10),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "validTest_transforms = transforms.Compose([transforms.Resize(size=258),\n",
    "                                           transforms.CenterCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=validTest_transforms)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=validTest_transforms)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "validLoader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=16,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# download VGG16 pretrained model\n",
    "model_transfer = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze parameters of the model to avoid brackpropagation\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# get the number of dog classes from the train_dataset\n",
    "number_of_dog_classes = len(train_dataset.classes)\n",
    "    \n",
    "# Define dog breed classifier part of model_transfer\n",
    "classifier = nn.Sequential(nn.Linear(25088, 4096),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.Linear(4096, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.Linear(512, number_of_dog_classes))\n",
    "\n",
    "# Rplace the original classifier with the dog breed classifier from above\n",
    "model_transfer.classifier = classifier\n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "# only train the classifier! -> model_transfer.classifier.parameters()\n",
    "optimizer_transfer = optim.Adam(model_transfer.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    print(f\"Batch Size: {loaders['train'].batch_size}\\n\")\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                print(f'Epoch:{epoch}/{n_epochs} \\tBatch:{batch_idx + 1}')\n",
    "                print(f'Train Loss: {train_loss}\\n')\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "           \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        # save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# define loaders_transfer\n",
    "loaders_transfer = {'train': trainLoader,\n",
    "                    'valid': validLoader,\n",
    "                    'test': testLoader}\n",
    "\n",
    "model_transfer = train(7, loaders_transfer, model_transfer, optimizer_transfer,\n",
    "                       criterion_transfer, use_cuda, 'model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)       \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        output = F.softmax(output, dim=1)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
    "\n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "#class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in train_dataset.classes]\n",
    "\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n",
    "\n",
    "def predict_breed_transfer(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Define normalization step for image\n",
    "    normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225))\n",
    "\n",
    "    # Define transformations of image\n",
    "    preprocess = transforms.Compose([transforms.Resize(258),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize])\n",
    "\n",
    "    # Preprocess image to 4D Tensor (.unsqueeze(0) adds a dimension)\n",
    "    img_tensor = preprocess(img).unsqueeze_(0)\n",
    "\n",
    "    # Move tensor to GPU if available\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.cuda()\n",
    "        \n",
    "    ## Inference\n",
    "    # Turn on evaluation mode\n",
    "    model_transfer.eval()\n",
    "    \n",
    "    # Get predicted category for image\n",
    "    with torch.no_grad():\n",
    "        output = model_transfer(img_tensor)\n",
    "        prediction = torch.argmax(output).item()\n",
    "        \n",
    "    # Turn off evaluation mode\n",
    "    model_transfer.train()\n",
    "    \n",
    "    # Use prediction to get dog breed\n",
    "    breed = class_names[prediction]\n",
    "    \n",
    "    return breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    if nn_face_detector(img_path):\n",
    "        print('Hello Human!')\n",
    "        plt.imshow(Image.open(img_path))\n",
    "        plt.show()\n",
    "        print(f'You look like a ... {predict_breed_transfer(img_path)}')\n",
    "        print('\\n-----------------------------------\\n')\n",
    "    elif dog_detector(img_path):\n",
    "        plt.imshow(Image.open(img_path))\n",
    "        plt.show()\n",
    "        print(f'This is a picture of a ... {predict_breed_transfer(img_path)}')\n",
    "        print('\\n-----------------------------------\\n')\n",
    "    else:\n",
    "        plt.imshow(Image.open(img_path))\n",
    "        plt.show()\n",
    "        print('Sorry, I did not detect a human or a dog in this image.')\n",
    "        print('\\n-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in np.hstack((human_files[:3], dog_files[:3])):\n",
    "    run_app(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames\n",
    "files = np.array(glob(\"my_images/*\"))\n",
    "for file_path in files:\n",
    "    run_app(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
